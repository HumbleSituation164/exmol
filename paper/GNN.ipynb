{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l8LOb8IbYxg"
   },
   "source": [
    "### GNN Model Code\n",
    "\n",
    "GNN model using molecular scent dataset from AI crowd (https://www.aicrowd.com/challenges/learning-to-smell)\n",
    "\n",
    "Code below modified from example code given in the \"Predicting DFT Energies with GNNs\" and \"Interpretability and Deep Learning\" sections of \"Deep Learning for Molecules and Materials\" textbook (https://whitead.github.io/dmol-book/applied/QM9.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to update CUDA_VISIBLE_DEVICES\n"
     ]
    }
   ],
   "source": [
    "print('Remember to update CUDA_VISIBLE_DEVICES')\n",
    "#For GPU nodes, edit value below based on allocated GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZeyyFPJbYxi",
    "outputId": "2ac9b9a9-ca8b-4608-d3a4-a2c8f937f71f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install matplotlib numpy pandas seaborn jax jaxlib dm-haiku tensorflow wandb optax\n",
    "\n",
    "#Code uses Weights & Biases\n",
    "import wandb\n",
    "#If running code in notebook & have not yet logged in w/it into W&B, uncomment lines below\n",
    "#wandb.login()\n",
    "#%env \"WANDB_NOTEBOOK_NAME\" \"GNN Model 4_Latest\"\n",
    "\n",
    "#Other imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import jax.experimental.optimizers as opt\n",
    "import pandas as pd\n",
    "import rdkit, rdkit.Chem, rdkit.Chem.rdDepictor, rdkit.Chem.Draw\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import haiku as hk\n",
    "import optax\n",
    "import sklearn.metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('dark',  {'xtick.bottom':True, 'ytick.left':True, 'xtick.color': '#666666', 'ytick.color': '#666666',\n",
    "                        'axes.edgecolor': '#666666', 'axes.linewidth':     0.8 , 'figure.dpi': 300})\n",
    "color_cycle = ['#1BBC9B', '#F06060', '#5C4B51', '#F3B562', '#6e5687']\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=color_cycle) \n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start a W&B run\n",
    "run = wandb.init(project='GNN_Model', entity='aseshad4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Save model inputs and hyperparameters\n",
    "config = wandb.config\n",
    "config.learning_rate = 1e-5\n",
    "config.num_Dense_layers = 2\n",
    "##NOTE: If changing # GNN layers, need to edit code in model_fn & code for storing parameters to take average of last 10 epochs\n",
    "config.num_GNN_layers = 4 \n",
    "config.numEpochs = 150\n",
    "config.steps_for_gradUpdate = 8\n",
    "config.graph_feat_length = 256\n",
    "config.node_feat_length = 256\n",
    "config.message_feat_length = config.node_feat_length\n",
    "config.weights_stddevGNN = 1e-2\n",
    "config.layerNormalization = True\n",
    "config.layerNormalization_edges = True\n",
    "config.edgeUpdates = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpwG4Qk0bYxl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load data --> file uploaded to jhub (locally stored)\n",
    "scentdata = pd.read_csv('train.csv')\n",
    "\n",
    "#Read in vocabulary text file --> this file gives all of the scent classes used in dataset\n",
    "file = open('vocabulary.txt')\n",
    "#Create list that stores all scent classes\n",
    "scentClasses = file.read().split('\\n')\n",
    "numClasses = len(scentClasses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3DUaY-hbYxm",
    "outputId": "2d3fa50a-7ec5-4e6a-e15f-7f43b711dba5"
   },
   "outputs": [],
   "source": [
    "def gen_smiles2graph(sml):\n",
    "    '''Argument for the RD2NX function should be a valid SMILES sequence\n",
    "    returns: the graph\n",
    "    '''\n",
    "    m = rdkit.Chem.MolFromSmiles(sml)\n",
    "    m = rdkit.Chem.AddHs(m)\n",
    "    order_string = {rdkit.Chem.rdchem.BondType.SINGLE: 1,\n",
    "                    rdkit.Chem.rdchem.BondType.DOUBLE: 2,\n",
    "                    rdkit.Chem.rdchem.BondType.TRIPLE: 3,\n",
    "                    rdkit.Chem.rdchem.BondType.AROMATIC: 4}\n",
    "    N = len(list(m.GetAtoms()))\n",
    "    nodes = np.zeros((N,config.node_feat_length))\n",
    "    for i in m.GetAtoms():\n",
    "        nodes[i.GetIdx(), i.GetAtomicNum()] = 1\n",
    "    \n",
    "    adj = np.zeros((N,N))\n",
    "    for j in m.GetBonds():\n",
    "        u = min(j.GetBeginAtomIdx(),j.GetEndAtomIdx())\n",
    "        v = max(j.GetBeginAtomIdx(),j.GetEndAtomIdx())        \n",
    "        order = j.GetBondType()\n",
    "        if order in order_string:\n",
    "            order = order_string[order]\n",
    "        else:\n",
    "            raise Warning('Ignoring bond order' + order)\n",
    "        adj[u, v] = 1       \n",
    "        adj[v, u] = 1 \n",
    "    adj += np.eye(N)\n",
    "    return nodes, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f32kIuxbYxo"
   },
   "outputs": [],
   "source": [
    "#Function that creates label vector given list of strings describing scent of molecule as input\n",
    "#Each index in label vector corresponds to specific scent -> if output has a 0 at index i, then molecule does not have scent i\n",
    "#If label vector has 1 at index i, then molecule does have scent i\n",
    "\n",
    "def createLabelVector(scentsList):\n",
    "    #Find class index in label vector that each scent corresponds to & update label for that molecule to 1\n",
    "    labelVector = np.zeros(numClasses)\n",
    "    for j in range(len(scentsList)):\n",
    "        #Find class index\n",
    "        classIndex = scentClasses.index(scentsList[j])\n",
    "        #print(classIndex)\n",
    "        #print(scentsList[j])\n",
    "        #print(scentClasses[classIndex])\n",
    "        #Update label vector\n",
    "        labelVector[classIndex] = 1\n",
    "    return labelVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68fbM76wbYxp"
   },
   "outputs": [],
   "source": [
    "def generateGraphs():\n",
    "    for i in range(len(scentdata)):\n",
    "        graph = gen_smiles2graph(scentdata.SMILES[i])   \n",
    "        tempScents = scentdata.SENTENCE[i].split(',') #Create list of strings describing scent of molecule i\n",
    "        labels = createLabelVector(tempScents)\n",
    "        yield graph, labels\n",
    "\n",
    "#Check that generateGraphs() works for 1st molecule\n",
    "#print(gen_smiles2graph(scentdata.SMILES[0]))\n",
    "#print(scentdata.SENTENCE[0].split(','))\n",
    "#print(np.nonzero(createLabelVector(scentdata.SENTENCE[0].split(','))))\n",
    "#print(scentClasses[89])\n",
    "data = tf.data.Dataset.from_generator(generateGraphs, output_types=((tf.float32, tf.float32), tf.float32), \n",
    "                                      output_shapes=((tf.TensorShape([None, config.node_feat_length]), tf.TensorShape([None, None])), tf.TensorShape([None])))\n",
    "\n",
    "#Generates graphs where molecule i is associated with molecule i+1's scent (scrambles scents & molecular features)\n",
    "#NOT correct way to generate graphs\n",
    "def generateGraphs_scrambled():\n",
    "    for i in range(len(scentdata)):\n",
    "        graph = gen_smiles2graph(scentdata.SMILES[i])\n",
    "        if(i+1 == len(scentdata)):\n",
    "            tempScents = scentdata.SENTENCE[0].split(',') #Create list of strings describing scent of molecule i\n",
    "        else:\n",
    "            tempScents = scentdata.SENTENCE[i+1].split(',') #Create list of strings describing scent of molecule i\n",
    "\n",
    "        labels = createLabelVector(tempScents)\n",
    "        yield graph, labels\n",
    "\n",
    "data_scrambled = tf.data.Dataset.from_generator(generateGraphs_scrambled, output_types=((tf.float32, tf.float32), tf.float32), \n",
    "                                      output_shapes=((tf.TensorShape([None, config.node_feat_length]), tf.TensorShape([None, None])), tf.TensorShape([None])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyI7FfjwbYxq"
   },
   "outputs": [],
   "source": [
    "numMolecules = len(scentdata.SMILES)\n",
    "print(f'Number of molecules: {numMolecules}')\n",
    "\n",
    "#Split data into training, testing & validation sets\n",
    "\n",
    "#shuffle dataset\n",
    "#shuffled_data = data.shuffle(numMolecules)\n",
    "\n",
    "train_N = int(numMolecules * 0.8)\n",
    "valid_N = int(numMolecules * 0.1)\n",
    "test_N = numMolecules - train_N - valid_N\n",
    "\n",
    "train_set = data.take(train_N)\n",
    "valid_set = data.skip(train_N).take(valid_N)\n",
    "test_set = data.skip(valid_N+train_N).take(test_N)\n",
    "\n",
    "#train_set_scrambled = data_scrambled.take(train_N)\n",
    "#valid_set_scrambled = data_scrambled.skip(train_N).take(valid_N)\n",
    "#test_set_scrambled = data_scrambled.skip(valid_N+train_N).take(test_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHYC-317bYxq"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    #yhat = jax.nn.sigmoid(logits)\n",
    "    return -jnp.mean(y * jnp.log(yhat + 1e-10) + (1 - y) * jnp.log(1 - yhat + 1e-10))\n",
    "\n",
    "def cross_entropy_logits(logits, y):\n",
    "    '''Cross entropy without sigmoid. Works with logits directly'''\n",
    "    return jnp.mean(jnp.clip(logits, 0, None) - logits * y + jnp.log(1 + jnp.exp(-jnp.abs(logits))))\n",
    "\n",
    "\n",
    "class GNNLayer(hk.Module): #TODO: If increase number of layers, stack features & new_features and shrink via dense layer\n",
    "\n",
    "    def __init__(self, output_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # split input into nodes, edges & features\n",
    "        nodes, edges, features = inputs\n",
    "        #Nodes is of shape (N, Nf) --> N = # atoms, Nf = node_feature_length\n",
    "        #Edges is of shape (N,N) (adjacency matrix)\n",
    "        #Features is of shape (Gf) --> Gf = graph_feature_length\n",
    "\n",
    "        graph_feature_len = features.shape[-1] #graph_feature_len (Gf)\n",
    "        node_feature_len = nodes.shape[-1] #node_feature_len (Nf)\n",
    "        message_feature_len = config.message_feat_length #message_feature_length (Mf)\n",
    "        \n",
    "        #Initialize weights\n",
    "        w_init = hk.initializers.RandomNormal(stddev = config.weights_stddevGNN)\n",
    "        \n",
    "        #we is of shape (Nf,Mf)\n",
    "        we = hk.get_parameter(\"we\", shape=[node_feature_len, message_feature_len], init=w_init)\n",
    "        \n",
    "        #b is of shape (Mf)\n",
    "        b = hk.get_parameter(\"b\", shape=[message_feature_len], init=w_init)\n",
    "        \n",
    "        #wv is of shape (Mf,Nf)\n",
    "        wv = hk.get_parameter(\"wv\", shape=[message_feature_len, node_feature_len], init=w_init)\n",
    "        \n",
    "        #wu is of shape (Nf,Gf)\n",
    "        wu = hk.get_parameter(\"wu\", shape=[node_feature_len, graph_feature_len], init=w_init)\n",
    "        \n",
    "        # make nodes be N x N x Nf so we can just multiply directly (N = number of atoms)\n",
    "        # ek is now shaped N x N x Mf\n",
    "        ek = jax.nn.leaky_relu(b + \n",
    "            jnp.repeat(nodes[jnp.newaxis,...], nodes.shape[0], axis=0) @ we * edges[..., None])\n",
    "\n",
    "        #ek *= edges[...,None]\n",
    "        \n",
    "        #Update edges, use jnp.any to have new_edges be of shape N x N\n",
    "        new_edges = jnp.any(ek, axis=-1)\n",
    "        \n",
    "        #Normalize over edge features w/layer normalization\n",
    "        new_edges = hk.LayerNorm(axis=[0,1], create_scale=False, create_offset=False, eps=1e-05)(new_edges)\n",
    "    \n",
    "        # take sum over neighbors to get ebar shape = Nf x Mf\n",
    "        ebar = jnp.sum(ek, axis=1)\n",
    "        \n",
    "        # dense layer for new nodes to get new_nodes shape = N x Nf\n",
    "        new_nodes = jax.nn.leaky_relu(ebar @ wv) + nodes\n",
    "        \n",
    "        #Normalize over node features w/layer normalization\n",
    "        new_nodes = hk.LayerNorm(axis=[0,1], create_scale=False, create_offset=False, eps=1e-05)(new_nodes)\n",
    "        \n",
    "        # sum over nodes to get shape features so global_node_features shape = Nf\n",
    "        global_node_features = jnp.sum(new_nodes, axis=0)\n",
    "        \n",
    "        # dense layer for new features so new_features shape = Gf\n",
    "        new_features = jax.nn.leaky_relu(global_node_features  @ wu) + features\n",
    "        \n",
    "        # just return features for ease of use\n",
    "        return new_nodes, new_edges, new_features\n",
    "\n",
    "    \n",
    "def model_fn(x):\n",
    "    nodes, edges = x\n",
    "    features = jnp.ones(config.graph_feat_length)\n",
    "    x = nodes, edges, features\n",
    "    \n",
    "    #NOTE: If edited config.num_GNN_layers, need to edit code below (increase or decrease # times have x = GNNLayer(...))\n",
    "    # 4 GNN layers\n",
    "    x = GNNLayer(output_size=config.graph_feat_length)(x)\n",
    "    x = GNNLayer(output_size=config.graph_feat_length)(x)\n",
    "    x = GNNLayer(output_size=config.graph_feat_length)(x)\n",
    "    x = GNNLayer(output_size=config.graph_feat_length)(x)\n",
    "    \n",
    "    # 2 dense layers\n",
    "    logits = hk.Linear(numClasses)(x[-1])\n",
    "    logits = hk.Linear(numClasses)(logits)\n",
    "\n",
    "    return logits #Model now returns logits\n",
    "\n",
    "model = hk.without_apply_rng(hk.transform(model_fn))\n",
    "\n",
    "#Use loss function below if model outputs yhat directly (not logits)\n",
    "def loss_fn(params, x, y):    \n",
    "    yhat = model.apply(params, x)\n",
    "    return cross_entropy(yhat, y) \n",
    "\n",
    "#Use loss function below if model outputs logits\n",
    "def loss_fn_logits(params, x, y):    \n",
    "    logits = model.apply(params, x)\n",
    "    return cross_entropy_logits(logits, y)\n",
    "\n",
    "#Accuracy function where accuracy is measured as |intersection of true and predicted labels|/|union of true and predicted labels|\n",
    "def accuracy_fn(params, x, y): \n",
    "    yhat = model.apply(params, x)\n",
    "    true_scentIndices = jnp.nonzero(y)\n",
    "    \n",
    "    # convert from prob to hard class -> positive yhat -> yhat = 1, else 0\n",
    "    hard_yhat = np.where(yhat > 0, np.ones_like(yhat), np.zeros_like(yhat))\n",
    "    predicted_scentIndices = jnp.nonzero(hard_yhat)\n",
    "    correctlyPredicted = len(np.intersect1d(predicted_scentIndices, true_scentIndices))\n",
    "    numTrueLabels = np.size(true_scentIndices)\n",
    "\n",
    "    #Total number of labels = number of labels in union of predicted & actual/true labels set\n",
    "    ##The size of this set = Actual labels - those correctly predicted + all predicted labels\n",
    "    numPredLabels = np.size(predicted_scentIndices)\n",
    "    totalLabels = numTrueLabels - correctlyPredicted + numPredLabels\n",
    "    return correctlyPredicted/totalLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bD3QGlOrbYxw"
   },
   "outputs": [],
   "source": [
    "#Compute competition accuracy\n",
    "#accuracy is measured as |intersection of true and predicted labels for top 3 predictions|/|union of true and predicted labels for top 3 predictions|\n",
    "def competitionAccuracy_fn(params, x, y): \n",
    "    yhat = model.apply(params, x)\n",
    "    numTrueLabels = jnp.count_nonzero(y) \n",
    "    true_scentIndices = jnp.nonzero(y)\n",
    "    pred_sortedIndices = np.argsort(yhat)\n",
    "\n",
    "    top15Pred = pred_sortedIndices[len(pred_sortedIndices)-15:]\n",
    "    #Create array storing top 5 predictions\n",
    "    predictions = np.zeros((5,3))\n",
    "    for j in range(5):\n",
    "        index = 15 - (j+1)*3\n",
    "        predictions[j] = top15Pred[index:15-j*3]\n",
    "        \n",
    "    numCorrect = np.empty(5)\n",
    "    for k in range(5):\n",
    "        numCorrect[k] = len(np.intersect1d(predictions[k], true_scentIndices))\n",
    "   \n",
    "    topNumCorrect = np.amax(numCorrect)\n",
    "    topPredictionSet = np.argmax(numCorrect)\n",
    "    #Total number of labels is number of labels in union of predicted & actual/true labels set (keep only 3 true labels)\n",
    "    ##The size of this set = Predicted labels - those accounted for in actual labels + all predicted labels\n",
    "    totalLabels = 3 + (3-topNumCorrect)\n",
    "    accuracyComp = topNumCorrect/totalLabels\n",
    "    return accuracyComp, topPredictionSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WkcIX4_bYxs",
    "outputId": "eb398112-a6a5-49e9-dbdd-172f63977410"
   },
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "sampleData = data.take(1)\n",
    "print('Using Regular Dataset')\n",
    "\n",
    "#sampleData_scrambled = data_scrambled.take(1)\n",
    "#print('Using Scrambled Dataset')\n",
    "\n",
    "for dataVal in sampleData: #Look into later how to get larger set\n",
    "    (nodes_i, edges_i), yi = dataVal\n",
    "nodes_i = nodes_i.numpy()\n",
    "edges_i = edges_i.numpy()\n",
    "\n",
    "yi = yi.numpy()\n",
    "xi = (nodes_i,edges_i)\n",
    "\n",
    "params = model.init(rng, xi)\n",
    "\n",
    "opt_init, opt_update = optax.chain(optax.apply_every(k=config.steps_for_gradUpdate), optax.adam(config.learning_rate))\n",
    "\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "@jax.jit\n",
    "def update(opt_state, x, y, params):\n",
    "    value, grads = jax.value_and_grad(loss_fn_logits)(params, x, y)\n",
    "    updates, opt_state = opt_update(grads, opt_state)\n",
    "    updated_params = optax.apply_updates(params, updates)\n",
    "    return value, opt_state, updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs = config.numEpochs\n",
    "print(f'Number of Epochs: {epochs}, learning rate: {config.learning_rate}, node_feature_len: {config.node_feat_length}, graph_feature_len: {config.graph_feat_length}, message_feature_length: {config.message_feat_length}, {config.num_Dense_layers} Dense, {config.num_GNN_layers} GNN layers')\n",
    "val_loss = np.zeros(epochs)\n",
    "val_accuracy = np.zeros(epochs)\n",
    "val_accComp = np.zeros(epochs)\n",
    "train_loss = np.zeros(epochs)\n",
    "train_accuracy = np.zeros(epochs)\n",
    "train_accComp = np.zeros(epochs)\n",
    "\n",
    "#Create arrays to store parameters for last 10 epochs\n",
    "pastTenParams_denseLayer1_w = np.zeros((10,config.graph_feat_length, numClasses))\n",
    "pastTenParams_denseLayer1_b = np.zeros((10,numClasses))\n",
    "pastTenParams_denseLayer2_w = np.zeros((10,numClasses, numClasses))\n",
    "pastTenParams_denseLayer2_b = np.zeros((10,numClasses))\n",
    "\n",
    "#NOTE: If edited config.num_GNN_layers, need to edit code below (add or remove pastTenParams_GNNLayerN...)\n",
    "pastTenParams_GNNLayer1_b = np.zeros((10,config.message_feat_length))\n",
    "pastTenParams_GNNLayer1_we = np.zeros((10,config.node_feat_length,config.message_feat_length))\n",
    "pastTenParams_GNNLayer1_wu = np.zeros((10,config.node_feat_length, config.graph_feat_length))\n",
    "pastTenParams_GNNLayer1_wv = np.zeros((10,config.message_feat_length,config.node_feat_length))\n",
    "pastTenParams_GNNLayer2_b = np.zeros((10,config.message_feat_length))\n",
    "pastTenParams_GNNLayer2_we = np.zeros((10,config.node_feat_length,config.message_feat_length))\n",
    "pastTenParams_GNNLayer2_wu = np.zeros((10,config.node_feat_length, config.graph_feat_length))\n",
    "pastTenParams_GNNLayer2_wv = np.zeros((10,config.message_feat_length,config.graph_feat_length))\n",
    "pastTenParams_GNNLayer3_b = np.zeros((10,config.message_feat_length))\n",
    "pastTenParams_GNNLayer3_we = np.zeros((10,config.node_feat_length,config.message_feat_length))\n",
    "pastTenParams_GNNLayer3_wu = np.zeros((10,config.node_feat_length, config.graph_feat_length))\n",
    "pastTenParams_GNNLayer3_wv = np.zeros((10,config.message_feat_length,config.graph_feat_length))\n",
    "pastTenParams_GNNLayer4_b = np.zeros((10,config.message_feat_length))\n",
    "pastTenParams_GNNLayer4_we = np.zeros((10,config.node_feat_length,config.message_feat_length))\n",
    "pastTenParams_GNNLayer4_wu = np.zeros((10,config.node_feat_length, config.graph_feat_length))\n",
    "pastTenParams_GNNLayer4_wv = np.zeros((10,config.message_feat_length,config.graph_feat_length))\n",
    "\n",
    "#early stopping counter\n",
    "#counter = 0\n",
    "#epochStoppedAt = epochs\n",
    "\n",
    "for e in range(epochs):\n",
    "    #if(counter == 3):\n",
    "       # print('early stopping')\n",
    "       # epochStoppedAt = e\n",
    "       # break #Early stopping\n",
    "    for i, elementInTrainSet in enumerate(train_set):\n",
    "        (ni,ei), yi = elementInTrainSet\n",
    "        ni = ni.numpy()\n",
    "        ei = ei.numpy()\n",
    "        yi = yi.numpy()\n",
    "        xi = ni,ei\n",
    "        value, opt_state, params = update(opt_state, xi, yi, params)\n",
    "        train_loss[e] += value\n",
    "        train_accuracy[e] += accuracy_fn(params, xi,yi)\n",
    "        accComp,index = competitionAccuracy_fn(params,xi,yi)\n",
    "        train_accComp[e] += accComp\n",
    "              \n",
    "    train_loss[e] = train_loss[e]/ train_N #Take average loss over all molecules\n",
    "    train_accuracy[e] = train_accuracy[e]/train_N\n",
    "    train_accComp[e] = train_accComp[e]/train_N\n",
    "    print(f'Training Loss, Epoch {e}: {train_loss[e]}')\n",
    "    print(f'Training Comp. Accuracy, Epoch{e}: {train_accComp[e]}')\n",
    "   \n",
    "    for j, v in enumerate(valid_set):\n",
    "        (n_val,e_val), y = v\n",
    "        n_val = n_val.numpy()\n",
    "        e_val = e_val.numpy()\n",
    "        y = y.numpy()\n",
    "        x = n_val,e_val\n",
    "        if (e >= epochs - 10): #Store last 10 parameters\n",
    "            paramsArr = jax.tree_util.tree_flatten(params)\n",
    "            pos = 10 - (epochs - e)\n",
    "            #NOTE: If edited config.num_GNN_layers, need to edit code below (add or remove assignment to pastTenParams_GNNLayer)\n",
    "            #In paramsArr, GNN layers are first sets of 4 parameters (b,we,wu,wv), then have parameters for Dense layers\n",
    "            pastTenParams_GNNLayer1_b[pos] = paramsArr[0][0]\n",
    "            pastTenParams_GNNLayer1_we[pos] = paramsArr[0][1]\n",
    "            pastTenParams_GNNLayer1_wu[pos] = paramsArr[0][2]\n",
    "            pastTenParams_GNNLayer1_wv[pos] = paramsArr[0][3]\n",
    "            pastTenParams_GNNLayer2_b[pos] = paramsArr[0][4]\n",
    "            pastTenParams_GNNLayer2_we[pos] = paramsArr[0][5]\n",
    "            pastTenParams_GNNLayer2_wu[pos] = paramsArr[0][6]\n",
    "            pastTenParams_GNNLayer2_wv[pos] = paramsArr[0][7]\n",
    "            pastTenParams_GNNLayer3_b[pos] = paramsArr[0][8]\n",
    "            pastTenParams_GNNLayer3_we[pos] = paramsArr[0][9]\n",
    "            pastTenParams_GNNLayer3_wu[pos] = paramsArr[0][10]\n",
    "            pastTenParams_GNNLayer3_wv[pos] = paramsArr[0][11]\n",
    "            pastTenParams_GNNLayer4_b[pos] = paramsArr[0][12]\n",
    "            pastTenParams_GNNLayer4_we[pos] = paramsArr[0][13]\n",
    "            pastTenParams_GNNLayer4_wu[pos] = paramsArr[0][14]\n",
    "            pastTenParams_GNNLayer4_wv[pos] = paramsArr[0][15]\n",
    "            pastTenParams_denseLayer1_b[pos] = paramsArr[0][16]\n",
    "            pastTenParams_denseLayer1_w[pos] = paramsArr[0][17]\n",
    "            pastTenParams_denseLayer2_b[pos] = paramsArr[0][18]\n",
    "            pastTenParams_denseLayer2_w[pos] = paramsArr[0][19]\n",
    "\n",
    "        loss = loss_fn_logits(params, x, y)\n",
    "        val_loss[e] += loss\n",
    "        val_accuracy[e] += accuracy_fn(params,x,y)\n",
    "        vAccComp, vIndex = competitionAccuracy_fn(params,x,y)\n",
    "        val_accComp[e] += vAccComp\n",
    "    \n",
    "    val_loss[e] = val_loss[e] / valid_N #Take average loss over all molecules\n",
    "    val_accuracy[e] = val_accuracy[e] / valid_N\n",
    "    val_accComp[e] = val_accComp[e] / valid_N\n",
    "    \n",
    "    #if (e > 0 and val_loss[e] > prevValidLoss): #Check if have increase in validation loss (early stopping)\n",
    "    #    counter += 1\n",
    "    #else:\n",
    "    #    counter = 0\n",
    "    #prevValidLoss = val_loss[e]\n",
    "\n",
    "    print(f'Epoch {e}, Validation Loss: {val_loss[e]}')\n",
    "    print(f'Validation Comp. Accuracy, Epoch{e}: {val_accComp[e]}')\n",
    "    \n",
    "    # 3. Log metrics over time to visualize performance (using Weights & Biases)\n",
    "    wandb.log({'Training loss': train_loss[e], 'Epoch': e})   \n",
    "    wandb.log({\"Training accuracy (competition accuracy)\": train_accComp[e], 'Epoch': e})    \n",
    "    wandb.log({\"Training accuracy (standard accuracy)\": train_accuracy[e], 'Epoch': e})    \n",
    "    wandb.log({\"Validation loss\": val_loss[e], 'Epoch': e})    \n",
    "    wandb.log({\"Validation accuracy (competition accuracy)\": val_accComp[e], 'Epoch': e})    \n",
    "    wandb.log({\"Validation accuracy (standard accuracy)\": val_accuracy[e], 'Epoch': e})    \n",
    "    \n",
    "    \n",
    "#save W&B run\n",
    "run.save()\n",
    "runName = run.name\n",
    "\n",
    "opt_params = params\n",
    "#Save optimal parameters\n",
    "opt_params_flattened = jax.tree_util.tree_flatten(opt_params)\n",
    "fileName = f'optParams_{epochs}Epochs_{runName}.npy'\n",
    "jnp.save(fileName, opt_params_flattened[0])\n",
    "\n",
    "#NOTE: If edited config.num_GNN_layers, need to edit code below (add/remove opt_params_GNNN_b/we/wu/wv = jnp.mean(...))\n",
    "#Take average of parameter values for last ten epochs & use them as weights\n",
    "opt_params_dense1_b = jnp.mean(pastTenParams_denseLayer1_b,0)\n",
    "opt_params_dense1_w = jnp.mean(pastTenParams_denseLayer1_w,0)\n",
    "opt_params_dense2_b = jnp.mean(pastTenParams_denseLayer2_b,0)\n",
    "opt_params_dense2_w = jnp.mean(pastTenParams_denseLayer2_w,0)\n",
    "opt_params_GNN1_b = jnp.mean(pastTenParams_GNNLayer1_b,0)\n",
    "opt_params_GNN1_we = jnp.mean(pastTenParams_GNNLayer1_we,0)\n",
    "opt_params_GNN1_wu = jnp.mean(pastTenParams_GNNLayer1_wu,0)\n",
    "opt_params_GNN1_wv = jnp.mean(pastTenParams_GNNLayer1_wv,0)\n",
    "opt_params_GNN2_b = jnp.mean(pastTenParams_GNNLayer2_b,0)\n",
    "opt_params_GNN2_we = jnp.mean(pastTenParams_GNNLayer2_we,0)\n",
    "opt_params_GNN2_wu = jnp.mean(pastTenParams_GNNLayer2_wu,0)\n",
    "opt_params_GNN2_wv = jnp.mean(pastTenParams_GNNLayer2_wv,0)\n",
    "opt_params_GNN3_b = jnp.mean(pastTenParams_GNNLayer3_b,0)\n",
    "opt_params_GNN3_we = jnp.mean(pastTenParams_GNNLayer3_we,0)\n",
    "opt_params_GNN3_wu = jnp.mean(pastTenParams_GNNLayer3_wu,0)\n",
    "opt_params_GNN3_wv = jnp.mean(pastTenParams_GNNLayer3_wv,0)\n",
    "opt_params_GNN4_b = jnp.mean(pastTenParams_GNNLayer4_b,0)\n",
    "opt_params_GNN4_we = jnp.mean(pastTenParams_GNNLayer4_we,0)\n",
    "opt_params_GNN4_wu = jnp.mean(pastTenParams_GNNLayer4_wu,0)\n",
    "opt_params_GNN4_wv = jnp.mean(pastTenParams_GNNLayer4_wv,0)\n",
    "\n",
    "#NOTE: If edited config.num_GNN_layers, need to edit line below (need to add/remove 'gnn_layer_N':{'b': opt_params_...., 'wu': ...})\n",
    "opt_params_avg =  {'gnn_layer': {'b': opt_params_GNN1_b, 'we': opt_params_GNN1_we, 'wu': opt_params_GNN1_wu, 'wv': opt_params_GNN1_wv},'gnn_layer_1': {'b': opt_params_GNN2_b, 'we': opt_params_GNN2_we, 'wu': opt_params_GNN2_wu, 'wv': opt_params_GNN2_wv}, 'gnn_layer_2': {'b': opt_params_GNN3_b, 'we': opt_params_GNN3_we, 'wu': opt_params_GNN3_wu, 'wv': opt_params_GNN3_wv}, 'gnn_layer_3': {'b': opt_params_GNN4_b, 'we': opt_params_GNN4_we, 'wu': opt_params_GNN4_wu, 'wv': opt_params_GNN4_wv}, 'linear': {'b': opt_params_dense1_b, 'w': opt_params_dense1_w}, 'linear_1': {'b': opt_params_dense2_b, 'w': opt_params_dense2_w}}\n",
    "\n",
    "#Save average of last 10 parameters\n",
    "opt_params_avg_flattened = jax.tree_util.tree_flatten(opt_params_avg)\n",
    "fileName_avg = f'optParamsAvg_{epochs}Epochs_{runName}.npy'\n",
    "jnp.save(fileName_avg, opt_params_avg_flattened[0])\n",
    "\n",
    "#Uncomment lines below to create plots & save them\n",
    "'''\n",
    "print('List of Plots Created & Saved: ')\n",
    "plt.title('GNN Loss vs Epoch')\n",
    "plt.plot(range(epochStoppedAt), val_loss[:epochStoppedAt], label = 'Validation Loss')\n",
    "plt.plot(range(epochStoppedAt), train_loss[:epochStoppedAt], label = 'Training Loss' )\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('GNNLossPlot_721_2Dense2Graph_GraphFeature256NodeFeature256AvgMessage1000Epoch.jpg')\n",
    "plt.savefig('GNNLossPlot_721_2Dense2Graph_GraphFeature256NodeFeature256AvgMessage1000Epoch.jpg')\n",
    "plt.close()\n",
    "\n",
    "plt.title('GNN Accuracy vs Epoch')\n",
    "plt.plot(range(epochStoppedAt), val_accuracy[:epochStoppedAt], label = 'Validation')\n",
    "plt.plot(range(epochStoppedAt), train_accuracy[:epochStoppedAt], label = 'Training' )\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('GNNAccuracyPlot_721_2Dense2Graph_GraphFeature256NodeFeature256AvgMessage1000Epoch.jpg')\n",
    "plt.savefig('GNNAccuracyPlot_721_2Dense2Graph_GraphFeature256NodeFeature256AvgMessage1000Epoch.jpg')\n",
    "plt.close()\n",
    "\n",
    "plt.title('GNN Competition Accuracy vs Epoch')\n",
    "plt.plot(range(epochStoppedAt), val_accComp[:epochStoppedAt], label = 'Validation')\n",
    "plt.plot(range(epochStoppedAt), train_accComp[:epochStoppedAt], label = 'Training' )\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Competition Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('GNNCompAccuracyPlot_721_2Dense2Graph_GraphFeature256NodeFeature256AvgMessage1000Epoch.jpg')\n",
    "plt.savefig('GNNCompAccuracyPlot_721_2Dense2Graph_GraphFeature256NodeFeature256AvgMessage1000Epoch.jpg')\n",
    "plt.close()\n",
    "\n",
    "#Plot validation loss & training loss from 8th epoch onwards\n",
    "plt.plot(range(epochStoppedAt)[8:epochStoppedAt], val_loss[8:epochStoppedAt], label='Validation Loss')\n",
    "plt.plot(range(epochStoppedAt)[8:epochStoppedAt], train_loss[8:epochStoppedAt], label='Training Loss')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title(f'GNN Training & Validation Loss, Epochs 8-{epochStoppedAt}')\n",
    "plt.show()\n",
    "print('GNNLossPlotPast8Epoch_721_2Dense2Graph_GraphFeature256NodeFeature256AvgMessage1000Epoch.jpg')\n",
    "plt.savefig('GNNLossPlotPast8Epoch_721_2Dense2Graph_GraphFeature256NodeFeature256AvgMessage1000Epoch.jpg')\n",
    "plt.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Di38bPxrbYxv"
   },
   "outputs": [],
   "source": [
    "#Compute accuracy (standard) on test set\n",
    "acc = np.zeros(test_N)\n",
    "accAverageParams = np.zeros(test_N)\n",
    "for i, testVal in enumerate(test_set):\n",
    "    (nodes_i, edges_i), yi = testVal\n",
    "    nodes_i = nodes_i.numpy()\n",
    "    edges_i = edges_i.numpy()\n",
    "    yi = yi.numpy()\n",
    "    xi = nodes_i,edges_i\n",
    "    accuracy = accuracy_fn(opt_params, xi,yi)\n",
    "    accuracyAverageParams = accuracy_fn(opt_params_avg, xi, yi)\n",
    "    #print(accuracy)\n",
    "    acc[i] = accuracy\n",
    "    accAverageParams[i] = accuracyAverageParams\n",
    "\n",
    "print(f'Overall Accuracy: {np.mean(acc)}')\n",
    "wandb.run.summary[\"Test set accuracy (standard accuracy)\"] = np.mean(acc)\n",
    "print(f'Overall Accuracy, average of last 10 parameters: {np.mean(accAverageParams)}')\n",
    "wandb.run.summary[\"Test set accuracy, average params(standard accuracy)\"] =  np.mean(accAverageParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOYQQhiRbYxx"
   },
   "outputs": [],
   "source": [
    "#Compute competition accuracy on test set\n",
    "accCompetition = np.zeros(test_N)\n",
    "accComp_avg = np.zeros(test_N)\n",
    "topPredSet = np.empty(test_N)\n",
    "topPredSet_avg = np.empty(test_N)\n",
    "for i, testVal in enumerate(test_set):\n",
    "    (nodes_i, edges_i), yi = testVal\n",
    "    nodes_i = nodes_i.numpy()\n",
    "    edges_i = edges_i.numpy()\n",
    "    yi = yi.numpy()\n",
    "    xi = nodes_i,edges_i\n",
    "    accuracy, topPredSet[i] = competitionAccuracy_fn(opt_params, xi,yi)\n",
    "    accuracy_avgParams, topPredSet_avg[i] = competitionAccuracy_fn(opt_params_avg, xi,yi)\n",
    "    #print(accuracy)\n",
    "    accCompetition[i] = accuracy\n",
    "    accComp_avg[i] = accuracy_avgParams\n",
    "\n",
    "print(f'Overall Accuracy (Competition): {np.mean(accCompetition)}')\n",
    "wandb.run.summary[\"Test set accuracy (competition accuracy)\"] = np.mean(accCompetition)\n",
    "print(f'Overall Accuracy (Competition), used average of last 10 parameters: {np.mean(accComp_avg)}')\n",
    "wandb.run.summary[\"Test set accuracy, average params(competition accuracy)\"] = np.mean(accComp_avg)\n",
    "\n",
    "#Print out how often the first 3 predictions were the best when computing competition accuracy\n",
    "top3Pred_correctPercent = 100*((test_N - np.count_nonzero(topPredSet))/test_N)\n",
    "top3Pred_correctPercent_avg = 100*((test_N - np.count_nonzero(topPredSet_avg))/test_N)\n",
    "print(f'Top 3 predictions were best on average (competition accuracy, regular params): {top3Pred_correctPercent}%')\n",
    "print(f'Top 3 predictions were best on average (competition accuracy, avg of last 10 params): {top3Pred_correctPercent_avg}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBgB_3tPbYxx",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Compute AUROC & Create ROC curve for each scent class - uses scikit-learn\n",
    "test_yhat = np.empty((test_N, numClasses)) #create empty array to store predictions on test set\n",
    "test_y = np.empty((test_N, numClasses))\n",
    "\n",
    "for i, testVal in enumerate(test_set):\n",
    "    (nodes_i, edges_i), yi = testVal\n",
    "    nodes_i = nodes_i.numpy()\n",
    "    edges_i = edges_i.numpy()\n",
    "    yi = yi.numpy()\n",
    "    xi = nodes_i,edges_i\n",
    "    test_yhat[i] = model.apply(opt_params, xi)\n",
    "    test_y[i] = yi\n",
    "\n",
    "#Count how often each scent occurs\n",
    "occurrences = np.zeros(numClasses)\n",
    "for i, val in enumerate(data):\n",
    "    (nodes_i, edges_i), yi = val\n",
    "    for j in range(numClasses):\n",
    "        occurrences[j] += yi[j]\n",
    "    \n",
    "aurocs_scikit = []\n",
    "aurocs_omitUncommonClasses = []\n",
    "for c in range(numClasses):\n",
    "    if(np.count_nonzero(test_y[:,c]) == 0):\n",
    "        print(f'Test set does not have any molecules with scent {scentClasses[c]}')\n",
    "    else:\n",
    "        ##Uncomment lines below to create ROC curves\n",
    "        #fpr, tpr, thresholds = sklearn.metrics.roc_curve(test_y[:,c], test_yhat[:,c])\n",
    "        #plt.plot(fpr, tpr, '-o', label='Trained Model')\n",
    "        #plt.plot([0,1], [0, 1], label='Naive Classifier')\n",
    "        #plt.ylabel('True Positive Rate')\n",
    "        #plt.xlabel('False Positive Rate')\n",
    "        #plt.title(f'ROC Curve for {scentClasses[c]}')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #plt.savefig(f'GNN_ROC_Curve_{scentClasses[c]}_{runName}.jpg')\n",
    "        #plt.close()\n",
    "        \n",
    "        auroc = sklearn.metrics.roc_auc_score(test_y[:,c], test_yhat[:,c])\n",
    "        aurocs_scikit.append(auroc)\n",
    "        if(occurrences[c] >= 30):\n",
    "            aurocs_omitUncommonClasses.append(auroc)\n",
    "            print(f'Included {scentClasses[c]}')\n",
    "        else:\n",
    "            print(f'Omitted {scentClasses[c]}')\n",
    "        print(f'AUROC for scent {scentClasses[c]}: {auroc}')\n",
    "\n",
    "mean_AUROC = np.mean(aurocs_scikit)\n",
    "mean_AUROC_omitUncommonScents = np.mean(aurocs_omitUncommonClasses)\n",
    "print(f'Mean AUROC: {mean_AUROC}')\n",
    "print(f'Mean AUROC (w/uncommon scent classes omitted): {mean_AUROC_omitUncommonScents}')\n",
    "wandb.run.summary['Mean AUROC'] = mean_AUROC\n",
    "wandb.run.summary['Mean AUROC w/uncommon scents omitted'] = mean_AUROC_omitUncommonScents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop W&B run\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GNN Model 3_With Haiku.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (my-rdkit-env)",
   "language": "python",
   "name": "my-rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
